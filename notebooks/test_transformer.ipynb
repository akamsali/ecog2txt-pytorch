{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd36239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ecog2txt_pytorch.dataloaders import EcogDataLoader\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc11cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"400\"\n",
    "tfrecord_path=\"/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400\"\n",
    "block_config_path=\"/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/conf/block_breakdowns.json\"\n",
    "manifest_path=\"/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/conf/mocha-1_word_sequence.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ea0670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package conflict (probably because you are using TF2.x)...not loading tfmpl...\n",
      "Warning: package 'samplerate' not found; skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gilbreth/akamsali/Research/research_env/lib64/python3.6/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import ecog2txt.data_generators\n",
    "with open(manifest_path, \"r\") as f:\n",
    "    manifest_file = yaml.load(f)\n",
    "    manifest_obj = manifest_file[int(subject_id)]\n",
    "\n",
    "_DG_kwargs = {}\n",
    "json_dir = manifest_obj['json_dir']\n",
    "DataGenerator = manifest_obj['DataGenerator']\n",
    "data_generator = DataGenerator(manifest_obj, subject_id, **dict(_DG_kwargs))\n",
    "\n",
    "num_channels = data_generator.num_ECoG_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d9d3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition_type training filtered_files ['/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B3.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B4.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B6.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B8.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B10.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B12.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B14.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B15.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B19.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B23.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B28.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B30.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B38.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B40.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B42.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B46.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B57.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B61.tfrecord']\n",
      "partition_type extra filtered_files ['/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B73.tfrecord', '/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B82.tfrecord']\n",
      "partition_type validation filtered_files ['/scratch/gilbreth/akamsali/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/EFC400_B72.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "from ecog2txt_pytorch.vocabulary import Vocabulary\n",
    "\n",
    "word_seq_vocabulary = Vocabulary(\"/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/conf/vocab.mocha-timit.1806\")\n",
    "SRC_VOCAB_SIZE = num_channels\n",
    "TGT_VOCAB_SIZE = len(word_seq_vocabulary.words_ind_map)\n",
    "EMB_SIZE = num_channels\n",
    "PAD_IDX = word_seq_vocabulary.words_ind_map['<pad>']\n",
    "#print(\"pad_idx\", PAD_IDX)\n",
    "EOS_IDX = word_seq_vocabulary.words_ind_map['<EOS>']\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "WIN_SIZE = 1\n",
    "\n",
    "block_config_all = None\n",
    "with open(block_config_path) as bf:\n",
    "    block_config_all = json.load(bf)\n",
    "\n",
    "description = {\"audio_sequence\": \"float\", \"ecog_sequence\": \"float\",\n",
    "               \"text_sequence\": \"byte\", \"phoneme_sequence\": \"byte\"}\n",
    "ecog = EcogDataLoader(tfrecord_path, block_config_all[subject_id],\n",
    "                      subject_id, num_ECoG_channels=num_channels, description=description)\n",
    "#print(ecog.get_data_loader_for_blocks())\n",
    "\n",
    "\n",
    "train_dataloader = ecog.get_data_loader_for_blocks(batch_size=BATCH_SIZE, partition_type='training')\n",
    "test_dataloader = ecog.get_data_loader_for_blocks(batch_size=BATCH_SIZE, partition_type='extra')\n",
    "valid_dataloader = ecog.get_data_loader_for_blocks(batch_size=BATCH_SIZE, partition_type='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de744024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecog2txt_pytorch.models.single_subject_transformer import *\n",
    "#from longformer.longformer import LongformerSelfAttention, LongformerConfig\n",
    "import torch.nn.functional as F\n",
    "#from jgm_utils.toolbox import wer\n",
    "\n",
    "\n",
    "# longformer_config = LongformerConffg(attention_window=[WIN_SIZE] * NUM_ENCODER_LAYERS,\n",
    "#  attention_dilation=[1] * NUM_ENCODER_LAYERS,\n",
    "#  hidden_size=EMB_SIZE,\n",
    "#  num_attention_heads=NHEAD)\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
    " EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
    " FFN_HID_DIM)\n",
    "\n",
    "# for i, layer in enumerate(transformer.transformer_encoder.layers):\n",
    "#  layer.self_attn = LongformerSelfAttention(config=longformer_config, layer_id=i)\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf9de84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "  src_seq_len = src.shape[0]\n",
    "  tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "  tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "  src_mask = torch.zeros((src_seq_len, src_seq_len), device=device)\n",
    "  #making a mask with sliding window centred around i\n",
    "  # ind_src = torch.arange(src_seq_len+WIN_SIZE-1, dtype=torch.int64).unfold(0,WIN_SIZE,1) - WIN_SIZE/2\n",
    "  # ind_src[ind_src<0] = 0\n",
    "  # ind_src[ind_src>=src_seq_len] = src_seq_len - 1\n",
    "  # ind_src = ind_src.type(torch.int64)\n",
    "  # src_mask.scatter_(1,ind_src,1)\n",
    "  # print('SRC_MASK', src_mask, 'SRC_mask_shape', src_mask.shape)\n",
    "  # src_mask = src_mask.float().masked_fill(src_mask == 0, float('-inf')).masked_fill(src_mask == 1, float(0.0))\n",
    "\n",
    "  #print(\"src shape\", src.shape)\n",
    "  src_padding_mask = torch.zeros(src.shape[:-1], device=device).transpose(0, 1)\n",
    "  tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "  return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "def train_epoch(model, train_dataloaders, optimizer):\n",
    "  model.train()\n",
    "  losses = 0\n",
    "  cnt = 0\n",
    "  train_acc = 0\n",
    "  for idx, (src, tgt) in enumerate(train_dataloader):\n",
    "      #print('src_shape', src.shape, 'tgt_shape', tgt.shape)\n",
    "      cnt += 1\n",
    "      src = src.to(device)\n",
    "      tgt = tgt.to(device)\n",
    "\n",
    "      tgt_input = tgt[:-1, :]\n",
    "\n",
    "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "      logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    "                                src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      tgt_out = tgt[1:,:].type(torch.LongTensor).to(device)\n",
    "\n",
    "      loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      losses += loss.item()\n",
    "      \n",
    "  return losses / cnt\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "  model.eval()\n",
    "  losses = 0\n",
    "  cnt = 0\n",
    "  val_accuracy = 0\n",
    "  for idx, (src, tgt) in enumerate(val_dataloader):\n",
    "    cnt += 1\n",
    "    src = src.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "\n",
    "    tgt_input = tgt[:-1, :]\n",
    "\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "    logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    "                              src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "    preds = torch.argmax(logits, dim=2)\n",
    "\n",
    "\n",
    "    tgt_out = tgt[1:,:].type(torch.LongTensor).to(device)\n",
    "    \n",
    "    val_accuracy += ( torch.sum(tgt_out == preds) / (preds.shape[0] * preds.shape[1]) )\n",
    "    loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "    losses += loss.item()\n",
    "\n",
    "\n",
    "  return losses / cnt, val_accuracy / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d02d1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 3.784, Val loss: 3.107, Val accuracy: 0.124Epoch time = 82.334s\n",
      "Epoch: 2, Train loss: 2.474, Val loss: 2.594, Val accuracy: 0.133Epoch time = 79.725s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4485d7981c1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Val accuracy: {val_acc:.3f}\"\n\u001b[1;32m      8\u001b[0m           f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
      "\u001b[0;32m<ipython-input-7-1c2a9f764645>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, val_dataloader)\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/research_env/lib64/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/research_env/lib64/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/research_env/lib64/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/research_env/lib64/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ChainDataset only supports IterableDataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/research_env/lib64/python3.6/site-packages/tfrecord/reader.py\u001b[0m in \u001b[0;36mexample_loader\u001b[0;34m(data_path, index_path, description, shard, compression_type)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mextract_feature_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypename_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/research_env/lib64/python3.6/site-packages/tfrecord/reader.py\u001b[0m in \u001b[0;36mextract_feature_dict\u001b[0;34m(features, description, typename_mapping)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key {key} doesn't exist (select from {all_keys})!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mprocessed_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypename_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/research_env/lib64/python3.6/site-packages/tfrecord/reader.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(typename, typename_mapping, key)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypename_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             return process_feature(features[key], typename,\n\u001b[0;32m--> 138\u001b[0;31m                                    typename_mapping, key)\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         raise TypeError(f\"Incompatible type: features should be either of type \"\n",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/research_env/lib64/python3.6/site-packages/tfrecord/reader.py\u001b[0m in \u001b[0;36mprocess_feature\u001b[0;34m(feature, typename, typename_mapping, key)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minferred_typename\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"float_list\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minferred_typename\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"int64_list\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "  start_time = time.time()\n",
    "  train_loss = train_epoch(transformer, train_dataloader, optimizer)\n",
    "  end_time = time.time()\n",
    "  val_loss, val_acc = evaluate(transformer, valid_dataloader)\n",
    "  print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Val accuracy: {val_acc:.3f}\"\n",
    "          f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

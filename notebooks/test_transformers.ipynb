{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from ecog2txt_pytorch.dataloaders import EcogDataLoader\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "subject_id = \"400\"\n",
    "tfrecord_path=\"/Users/akshita/Documents/Research/Makin/data/ecog2txt/word_sequence/tf_records/EFC400/{}.tfrecord\"\n",
    "block_config_path=\"/Users/akshita/Documents/Research/Makin/ecog2txt-pytorch/conf/block_breakdowns.json\"\n",
    "manifest_path=\"/Users/akshita/Documents/Research/Makin/ecog2txt-pytorch/conf/mocha-1_word_sequence.yaml\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshita/Documents/Research/research_env/lib/python3.7/site-packages/ipykernel_launcher.py:5: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 2880x2880 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the number of channels\n",
    "import yaml\n",
    "import ecog2txt.data_generators\n",
    "with open(manifest_path, \"r\") as f:\n",
    "    manifest_file = yaml.load(f)\n",
    "    manifest_obj = manifest_file[int(subject_id)]\n",
    "\n",
    "_DG_kwargs = {}\n",
    "json_dir = manifest_obj['json_dir']\n",
    "DataGenerator = manifest_obj['DataGenerator']\n",
    "data_generator = DataGenerator(manifest_obj, subject_id, **dict(_DG_kwargs))\n",
    "\n",
    "num_channels = data_generator.num_ECoG_channels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<EOS>': 1, '<OOV>': 2, 'fangs_': 3, 'medieval_': 4, 'nothing_': 5, 'lightbulbs_': 6, 'antagonistic_': 7, 'favour_': 8, 'sure_': 9, 'subdued_': 10, 'signed_': 11, 'rewarded_': 12, 'take_': 13, 'flower_': 14, 'overwhelmed_': 15, 'occurs_': 16, 'caught_': 17, 'then_': 18, 'camp_': 19, 'window_': 20, 'stab_': 21, 'noteworthy_': 22, 'classical_': 23, 'nan_': 24, 'bells_': 25, 'idly_': 26, 'autumn_': 27, 'feelings_': 28, 'rescue_': 29, 'wrap_': 30, 'doll_': 31, 'capable_': 32, 'george_': 33, 'chablis_': 34, 'movie_': 35, 'watch_': 36, 'am_': 37, 'aptitude_': 38, 'worry_': 39, 'schooner_': 40, 'garbage_': 41, 'burned_': 42, 'bureaucracy_': 43, 'judge_': 44, 'view_': 45, 'diagram_': 46, 'mask_': 47, 'damage_': 48, 'scarf_': 49, 'zips_': 50, 'measured_': 51, 'necklace_': 52, 'participate_': 53, 'motorists_': 54, 'upbringing_': 55, 'objects_': 56, 'developing_': 57, 'cigarettes_': 58, 'dad_': 59, 'acts_': 60, 'paranoid_': 61, 'hires_': 62, 'slipped_': 63, 'emblem_': 64, 'makes_': 65, 'crucial_': 66, 'showers_': 67, 'square_': 68, 'red_': 69, 'scoop_': 70, 'kidnappers_': 71, 'tycoons_': 72, 'dime_': 73, 'dolphins_': 74, 'greatly_': 75, 'notoriety_': 76, 'degrees_': 77, 'discussions_': 78, 'jaw_': 79, 'film_': 80, 'disappeared_': 81, 'arm_': 82, 'exist_': 83, 'cheating_': 84, 'counted_': 85, 'teach_': 86, 'modelling_': 87, 'rodents_': 88, 'itemize_': 89, 'buying_': 90, 'contagious_': 91, 'jane_': 92, 'people_': 93, 'basketball_': 94, 'etiquette_': 95, 'prowler_': 96, 'symbols_': 97, 'generous_': 98, 'biologists_': 99, 'thursday_': 100, 'informative_': 101, 'crayons_': 102, 'students_': 103, 'of_': 104, 'overcharged_': 105, 'small_': 106, 'yesterday_': 107, 'sun_': 108, 'ideology_': 109, 'forms_': 110, 'out_': 111, 'murals_': 112, 'instructions_': 113, 'confirm_': 114, 'values_': 115, 'cooperation_': 116, 'aglow_': 117, 'street_': 118, 'so_': 119, 'more_': 120, 'todd_': 121, 'jim_': 122, 'robin_': 123, 'throughout_': 124, 'received_': 125, 'furrier_': 126, 'here_': 127, 'geological_': 128, 'first_': 129, 'bidding_': 130, 'exciting_': 131, 'attitude_': 132, 'impossible_': 133, 'costumes_': 134, 'lost_': 135, 'psychological_': 136, 'build_': 137, 'evening_': 138, 'leap_': 139, 'walk_': 140, 'academic_': 141, 'shimmered_': 142, 'hours_': 143, 'contributed_': 144, 'straight_': 145, 'cutbacks_': 146, 'burglar_': 147, 'on_': 148, 'cubic_': 149, 'authorization_': 150, 'failure_': 151, 'john_': 152, 'lessons_': 153, 'ankle_': 154, 'rose_': 155, 'carl_': 156, 'chipper_': 157, 'church_': 158, 'expertise_': 159, 'water_': 160, 'lifelong_': 161, 'abbreviate_': 162, 'requires_': 163, 'finish_': 164, 'reads_': 165, 'fish_': 166, 'michael_': 167, 'remove_': 168, 'bones_': 169, 'off_': 170, 'fundraisers_': 171, 'handbag_': 172, 'brother_': 173, 'gowns_': 174, 'woman_': 175, 'precaution_': 176, 'allowance_': 177, 'exquisite_': 178, 'outstanding_': 179, 'scampered_': 180, 'bracelet_': 181, 'natural_': 182, 'inferiority_': 183, 'freely_': 184, 'near_': 185, 'pickpocket_': 186, 'military_': 187, 'galoshes_': 188, 'cats_': 189, 'jaguars_': 190, 'alien_': 191, 'marvellously_': 192, 'effects_': 193, 'screw_': 194, 'classrooms_': 195, 'snow_': 196, 'muscles_': 197, 'business_': 198, 'one_': 199, 'addition_': 200, 'coast_': 201, 'related_': 202, 'status_': 203, 'wood_': 204, 'vaporization_': 205, 'programs_': 206, 'drunkard_': 207, 'display_': 208, 'glue_': 209, 'assume_': 210, 'rather_': 211, 'corsage_': 212, 'welfare_': 213, 'due_': 214, 'grow_': 215, 'animals_': 216, 'along_': 217, 'rachel_': 218, 'surplus_': 219, 'through_': 220, 'skirt_': 221, 'eyestrain_': 222, 'plate_': 223, 'helped_': 224, 'control_': 225, \"don't_\": 226, 'strongly_': 227, \"we'll_\": 228, 'microorganisms_': 229, 'toy_': 230, 'complex_': 231, 'thermometer_': 232, 'large_': 233, 'neglect_': 234, 'beds_': 235, 'decorate_': 236, 'paragraph_': 237, 'know_': 238, 'child_': 239, 'fail_': 240, 'slope_': 241, 'dispute_': 242, 'recuperating_': 243, 'technical_': 244, 'ready_': 245, 'chases_': 246, 'gunman_': 247, 'sugar_': 248, 'auburn_': 249, 'social_': 250, 'phony_': 251, 'would_': 252, 'plow_': 253, 'vapour_': 254, 'hat_': 255, 'activities_': 256, 'vietnamese_': 257, 'subway_': 258, 'recoiled_': 259, 'expense_': 260, 'buyer_': 261, 'refurbishing_': 262, 'begin_': 263, 'society_': 264, 'black_': 265, 'suburbanites_': 266, 'flurries_': 267, 'does_': 268, 'movies_': 269, 'spring_': 270, 'ice_': 271, 'coleslaw_': 272, 'activity_': 273, 'seldom_': 274, 'drugs_': 275, 'stronghold_': 276, 'discount_': 277, 'documents_': 278, 'penguins_': 279, 'zinnias_': 280, 'frost_': 281, 'are_': 282, 'argued_': 283, 'steaming_': 284, 'born_': 285, 'joyce_': 286, 'oily_': 287, 'ate_': 288, 'by_': 289, 'rag_': 290, 'vault_': 291, 'angry_': 292, 'mother_': 293, 'dance_': 294, 'five_': 295, 'spilled_': 296, 'save_': 297, 'thursdays_': 298, 'jokes_': 299, 'subtitles_': 300, 'chemicals_': 301, 'some_': 302, 'finding_': 303, 'hyena_': 304, 'jeep_': 305, 'garden_': 306, 'atypical_': 307, 'shadow_': 308, 'emergency_': 309, 'ringing_': 310, 'worn_': 311, 'strength_': 312, 'villains_': 313, 'precincts_': 314, 'reptiles_': 315, 'boy_': 316, 'dowager_': 317, 'year_': 318, 'two_': 319, 'drugstore_': 320, 'purple_': 321, 'cuisine_': 322, 'all_': 323, 'my_': 324, 'make_': 325, 'cornered_': 326, 'pays_': 327, 'steep_': 328, 'sweaters_': 329, 'non-profit_': 330, 'see_': 331, 'juice_': 332, 'musical_': 333, 'security_': 334, 'we_': 335, 'needed_': 336, 'became_': 337, 'outdoors_': 338, 'cartoon_': 339, 'hot_': 340, 'few_': 341, 'safari_': 342, 'stimulating_': 343, 'recent_': 344, 'famous_': 345, 'crab_': 346, 'cut_': 347, 'huge_': 348, 'lemon_': 349, 'bandaged_': 350, 'unlimited_': 351, 'verbalize_': 352, 'spherical_': 353, 'thinner_': 354, 'alfalfa_': 355, 'pickpockets_': 356, 'policy_': 357, 'discouraging_': 358, 'will_': 359, 'worried_': 360, 'well-kept_': 361, 'barracuda_': 362, 'simple_': 363, 'thread_': 364, 'progress_': 365, 'countryside_': 366, 'instruments_': 367, 'article_': 368, 'roll_': 369, 'desert_': 370, 'sheila_': 371, 'cooperates_': 372, 'lines_': 373, 'pearls_': 374, 'as_': 375, 'lawyers_': 376, 'experiment_': 377, 'sugars_': 378, 'sweet_': 379, 'frequent_': 380, 'solve_': 381, 'seismic_': 382, 'using_': 383, 'wall_': 384, 'smash_': 385, 'please_': 386, 'valuables_': 387, 'antarctic_': 388, 'peck_': 389, 'compliance_': 390, 'atheists_': 391, 'corduroy_': 392, 'york_': 393, 'charmer_': 394, 'pressure_': 395, 'most_': 396, \"couldn't_\": 397, 'fleecy_': 398, 'upgrade_': 399, 'masquerade_': 400, 'attacked_': 401, 'sleeping_': 402, 'appointed_': 403, 'unbeatable_': 404, 'thoroughbred_': 405, 'frequently_': 406, 'felt_': 407, 'cab_': 408, 'exam_': 409, 'diagnosis_': 410, 'much_': 411, 'further_': 412, 'learn_': 413, 'kayak_': 414, 'brush_': 415, 'oasis_': 416, 'elderly_': 417, 'goulash_': 418, 'corner_': 419, 'overlooked_': 420, 'ride_': 421, 'cast_': 422, 'force_': 423, 'essay_': 424, 'expensive_': 425, 'alimony_': 426, 'candy_': 427, 'planned_': 428, 'treat_': 429, 'light_': 430, 'co-exist_': 431, 'weatherproof_': 432, 'disclaimer_': 433, 'seeking_': 434, 'he_': 435, 'gab_': 436, 'now_': 437, 'soysauce_': 438, 'items_': 439, 'penalty_': 440, 'extra_': 441, 'contains_': 442, 'lagoon_': 443, 'guess_': 444, 'be_': 445, 'muscular_': 446, 'night_': 447, 'irving_': 448, 'exchanged_': 449, 'constantly_': 450, 'audience_': 451, 'clear_': 452, 'major_': 453, 'wear_': 454, 'run_': 455, 'horseradish_': 456, 'women_': 457, 'mango_': 458, 'answered_': 459, 'connoisseur_': 460, 'papaya_': 461, 'avoid_': 462, 'annoying_': 463, 'spurious_': 464, 'trauma_': 465, 'always_': 466, 'cat_': 467, 'each_': 468, 'likes_': 469, 'pie_': 470, 'cashmere_': 471, 'hook_': 472, 'feet_': 473, 'blouses_': 474, 'swing_': 475, 'cranberry_': 476, 'thanksgiving_': 477, 'shampooed_': 478, 'plan_': 479, 'serve_': 480, 'ambled_': 481, 'names_': 482, 'smiths_': 483, 'his_': 484, 'hyenas_': 485, 'cliff_': 486, 'grandmother_': 487, 'chop_': 488, 'nectar_': 489, 'eyedrops_': 490, 'nice_': 491, 'your_': 492, 'aquatic_': 493, 'sunshine_': 494, 'calico_': 495, 'put_': 496, 'carpet_': 497, 'nora_': 498, 'county_': 499, 'has_': 500, 'provoked_': 501, 'bank_': 502, 'departure_': 503, 'tim_': 504, 'interpretation_': 505, 'suggestion_': 506, 'jewels_': 507, 'shoes_': 508, 'must_': 509, 'wealth_': 510, 'medical_': 511, 'redwoods_': 512, 'loss_': 513, 'thick_': 514, 'costume_': 515, 'was_': 516, 'retracted_': 517, 'colored_': 518, 'moment_': 519, 'that_': 520, 'calf_': 521, 'based_': 522, 'choosing_': 523, 'urchins_': 524, 'evaluate_': 525, 'cured_': 526, 'petticoats_': 527, 'spotted_': 528, 'seattle_': 529, 'december_': 530, 'stinging_': 531, 'statuesque_': 532, 'bluejay_': 533, 'siamese_': 534, 'answer_': 535, 'consume_': 536, 'quite_': 537, 'miami_': 538, 'nearest_': 539, 'intelligible_': 540, 'tranquilizers_': 541, 'never_': 542, 'goat_': 543, 'cameo_': 544, 'ears_': 545, 'choices_': 546, 'available_': 547, 'home_': 548, 'sky_': 549, 'wealthy_': 550, 'zoos_': 551, 'porch_': 552, 'swedish_': 553, 'biblical_': 554, 'growing_': 555, 'and_': 556, 'angora_': 557, 'lamb_': 558, 'bonfire_': 559, 'audiovisual_': 560, 'plymouth_': 561, 'wardrobe_': 562, 'forgery_': 563, 'ocean_': 564, 'perpendicular_': 565, 'green_': 566, 'brie_': 567, 'times_': 568, 'cheap_': 569, 'hard_': 570, 'seesaw_': 571, 'exposure_': 572, 'barometric_': 573, 'forgot_': 574, 'wound_': 575, 'artists_': 576, 'bobcat_': 577, 'malnourished_': 578, 'public_': 579, 'several_': 580, 'greg_': 581, 'grades_': 582, 'standby_': 583, 'action_': 584, 'friends_': 585, 'making_': 586, 'might_': 587, 'clarification_': 588, 'icicles_': 589, 'underbrush_': 590, 'funding_': 591, 'therapy_': 592, 'tongue_': 593, 'farmers_': 594, 'vocabulary_': 595, 'need_': 596, 'poor_': 597, 'appreciated_': 598, 'danny_': 599, 'power_': 600, 'beans_': 601, 'lori_': 602, 'graph_': 603, 'surface_': 604, 'events_': 605, 'triumphant_': 606, 'fruit_': 607, 'bride_': 608, 'pairs_': 609, 'customer_': 610, 'hindu_': 611, 'predicament_': 612, 'contained_': 613, 'state_': 614, 'postdate_': 615, 'canteen_': 616, 'formula_': 617, 'keep_': 618, 'company_': 619, 'celebrates_': 620, 'hungarian_': 621, 'barbed_': 622, 'open_': 623, 'citizenship_': 624, 'enter_': 625, 'want_': 626, 'edge_': 627, 'these_': 628, 'muskrat_': 629, 'irish_': 630, 'approach_': 631, 'arriving_': 632, 'hood_': 633, 'gooseberry_': 634, 'phil_': 635, 'payments_': 636, 'sat_': 637, 'uses_': 638, 'fjords_': 639, 'ambiguous_': 640, 'emphasized_': 641, 'composure_': 642, 'shape_': 643, 'parties_': 644, 'ointment_': 645, 'began_': 646, 'gas_': 647, 'trespassing_': 648, 'repainting_': 649, 'present_': 650, 'legislature_': 651, 'rare_': 652, 'affirmative_': 653, 'myopia_': 654, 'completely_': 655, 'waste_': 656, 'groundhog_': 657, 'romantic_': 658, 'easy_': 659, 'needs_': 660, 'sundaes_': 661, 'flag_': 662, 'poison_': 663, 'unexpected_': 664, 'prevented_': 665, 'vegetable_': 666, 'shaving_': 667, 'aluminium_': 668, 'prescribe_': 669, 'deal_': 670, 'pronunciation_': 671, 'sudden_': 672, 'eating_': 673, 'shortage_': 674, 'execution_': 675, 'overweight_': 676, 'herb_': 677, 'primitive_': 678, 'house_': 679, 'best_': 680, 'curiosity_': 681, 'twice_': 682, 'ron_': 683, 'hull_': 684, 'graduation_': 685, 'accounts_': 686, 'caused_': 687, 'convenient_': 688, 'tunafish_': 689, 'destroy_': 690, 'paper_': 691, 'bright_': 692, 'flew_': 693, 'just_': 694, 'drift_': 695, 'files_': 696, 'sea_': 697, 'prison_': 698, 'waiting_': 699, 'beg_': 700, 'him_': 701, 'controlled_': 702, 'men_': 703, 'changes_': 704, 'teaspoons_': 705, 'products_': 706, 'gifts_': 707, 'avalanche_': 708, 'rarely_': 709, 'eleven_': 710, 'who_': 711, 'from_': 712, 'fog_': 713, 'appetizers_': 714, 'clay_': 715, 'lone_': 716, 'wore_': 717, 'wandered_': 718, 'yacht_': 719, 'over_': 720, 'outer_': 721, 'uninterrupted_': 722, 'habit_': 723, 'promote_': 724, 'agricultural_': 725, 'man_': 726, 'stew_': 727, 'lake_': 728, 'peeling_': 729, 'dirty_': 730, 'exotic_': 731, 'previous_': 732, 'potatoes_': 733, 'orders_': 734, 'dressing_': 735, 'turner_': 736, 'lodge_': 737, 'were_': 738, 'purchased_': 739, 'soon_': 740, 'temperate_': 741, 'serpent_': 742, 'features_': 743, 'cyclical_': 744, 'equipment_': 745, 'according_': 746, 'intelligent_': 747, 'work_': 748, 'heating_': 749, 'generals_': 750, 'zones_': 751, 'occasionally_': 752, 'speech_': 753, 'moth_': 754, 'proof_': 755, 'consuming_': 756, 'nine_': 757, 'bungalow_': 758, 'aviaries_': 759, 'honour_': 760, 'ducks_': 761, 'barb_': 762, 'shawn_': 763, 'bike_': 764, 'household_': 765, 'coincided_': 766, 'this_': 767, 'museum_': 768, 'jennifer_': 769, 'block_': 770, 'under_': 771, 'tribes_': 772, 'execute_': 773, 'gremlins_': 774, 'often_': 775, 'tears_': 776, 'heroism_': 777, 'those_': 778, 'calcium_': 779, 'play_': 780, 'dish_': 781, 'untimely_': 782, 'fascinating_': 783, 'blistered_': 784, 'club_': 785, 'artificial_': 786, 'chronological_': 787, 'food_': 788, 'careful_': 789, 'trish_': 790, 'drawing_': 791, 'idiotically_': 792, 'can_': 793, 'toothpaste_': 794, 'stopwatch_': 795, 'fixed_': 796, 'store_': 797, 'order_': 798, 'older_': 799, 'roger_': 800, 'tweezers_': 801, 'project_': 802, 'zircons_': 803, 'comes_': 804, 'orange_': 805, 'site_': 806, 'interchangeably_': 807, 'back_': 808, 'handle_': 809, 'regarding_': 810, 'same_': 811, 'nearly_': 812, 'tugboats_': 813, 'bottom_': 814, 'practical_': 815, 'corn_': 816, 'chose_': 817, 'early_': 818, 'neoclassic_': 819, 'course_': 820, 'cheese_': 821, 'dislikes_': 822, 'locked_': 823, 'get_': 824, 'well_': 825, 'fell_': 826, 'lot_': 827, 'way_': 828, 'theatre_': 829, 'set_': 830, 'tomorrow_': 831, 'others_': 832, 'guarantees_': 833, 'even_': 834, 'hired_': 835, 'smiles_': 836, 'evidence_': 837, 'suffer_': 838, 'combine_': 839, 'trees_': 840, 'shoulder_': 841, 'thomas_': 842, 'fructose_': 843, 'draw_': 844, 'elm_': 845, 'skill_': 846, 'stems_': 847, 'postponed_': 848, 'interior_': 849, 'tofu_': 850, 'answers_': 851, 'glistening_': 852, 'bugle_': 853, 'moisture_': 854, 'they_': 855, 'adjourned_': 856, 'welcome_': 857, 'ashtray_': 858, 'skirts_': 859, 'before_': 860, 'living_': 861, 'oysters_': 862, 'three_': 863, 'butterscotch_': 864, 'within_': 865, 'misplaced_': 866, 'instead_': 867, 'frightened_': 868, 'such_': 869, 'jam_': 870, 'lively_': 871, 'mum_': 872, 'there_': 873, 'distress_': 874, 'publicity_': 875, 'bog_': 876, 'remember_': 877, 'excluded_': 878, 'popular_': 879, 'money_': 880, 'sandwich_': 881, 'blue_': 882, 'rhythm_': 883, 'ability_': 884, 'updating_': 885, 'geese_': 886, 'week_': 887, 'zoologist_': 888, 'appliances_': 889, 'highway_': 890, 'glistened_': 891, 'which_': 892, 'pure_': 893, 'yards_': 894, 'made_': 895, 'anyone_': 896, 'silly_': 897, 'fortune_': 898, 'blues_': 899, 'turquoise_': 900, 'jungle-like_': 901, 'matched_': 902, 'aches_': 903, 'repertoire_': 904, 'path_': 905, 'appointment_': 906, 'tell_': 907, 'safe_': 908, 'roof_': 909, 'marine_': 910, 'below_': 911, 'colleges_': 912, 'zebras_': 913, 'breakfast_': 914, 'income_': 915, 'ironing_': 916, 'garage_': 917, 'vegetables_': 918, 'thing_': 919, 'use_': 920, 'removal_': 921, 'placed_': 922, 'hit_': 923, 'eight_': 924, 'survive_': 925, 'hear_': 926, 'miles_': 927, 'is_': 928, 'chloride_': 929, 'spanish_': 930, 'takes_': 931, 'birth_': 932, 'boston_': 933, 'excitement_': 934, 'many_': 935, 'or_': 936, 'chlorine_': 937, 'illegally_': 938, 'cartoons_': 939, 'number_': 940, 'explicitly_': 941, 'right_': 942, 'rug_': 943, 'drenched_': 944, 'looking_': 945, 'field_': 946, 'both_': 947, 'place_': 948, 'reading_': 949, 'when_': 950, 'clams_': 951, 'sketched_': 952, 'disease_': 953, 'gained_': 954, 'mirage_': 955, 'no_': 956, 'why_': 957, 'colourful_': 958, 'afternoon_': 959, 'skewers_': 960, 'clearly_': 961, 'obtain_': 962, 'problem_': 963, 'survey_': 964, 'agency_': 965, 'watches_': 966, 'outgrew_': 967, 'cheque_': 968, 'thin_': 969, 'smelled_': 970, 'diane_': 971, 'luxurious_': 972, 'stole_': 973, 'yet_': 974, 'invest_': 975, 'minor_': 976, 'sport_': 977, 'preparing_': 978, 'next_': 979, 'help_': 980, 'had_': 981, 'group_': 982, 'toddler_': 983, 'judged_': 984, 'milk_': 985, 'left_': 986, 'toxic_': 987, 'amoebas_': 988, 'meeting_': 989, 'alligators_': 990, 'across_': 991, 'abruptly_': 992, 'oriental_': 993, 'moon_': 994, 'gift_': 995, 'butcher_': 996, 'cupcakes_': 997, 'table_': 998, 'how_': 999, 'i_': 1000, 'their_': 1001, 'surely_': 1002, 'saw_': 1003, 'giant_': 1004, 'abdomen_': 1005, 'hats_': 1006, 'spray_': 1007, 'prospective_': 1008, 'the_': 1009, 'expression_': 1010, 'collects_': 1011, 'mayan_': 1012, 'where_': 1013, 'existing_': 1014, 'lack_': 1015, 'picked_': 1016, 'us_': 1017, 'correct_': 1018, 'latest_': 1019, 'cook_': 1020, 'stray_': 1021, 'spielberg_': 1022, 'hundred_': 1023, 'disguise_': 1024, 'axis_': 1025, 'obey_': 1026, 'surveying_': 1027, 'spend_': 1028, 'radioactive_': 1029, 'creole_': 1030, 'regular_': 1031, 'westchester_': 1032, 'ballet_': 1033, 'finds_': 1034, 'fawn_': 1035, 'rock-and-roll_': 1036, 'ideal_': 1037, 'severe_': 1038, 'cory_': 1039, 'ambidextrous_': 1040, 'processed_': 1041, 'pop_': 1042, 'gus_': 1043, 'assistance_': 1044, 'balls_': 1045, 'earthquake_': 1046, 'viewpoint_': 1047, 'sprained_': 1048, 'triggered_': 1049, 'data_': 1050, 'broken_': 1051, 'endurance_': 1052, 'consists_': 1053, 'tax_': 1054, 'gregory_': 1055, 'nancy_': 1056, 'crooked_': 1057, 'operates_': 1058, 'escalator_': 1059, 'once_': 1060, 'causeway_': 1061, 'holidays_': 1062, 'opens_': 1063, 'development_': 1064, 'soothed_': 1065, 'finger_': 1066, 'ignored_': 1067, 'become_': 1068, 'undeniably_': 1069, 'beautiful_': 1070, 'opaque_': 1071, 'fill_': 1072, 'reminded_': 1073, 'singer_': 1074, 'rhubarb_': 1075, 'economic_': 1076, 'giraffes_': 1077, 'new_': 1078, 'brightly_': 1079, 'lengthy_': 1080, 'temper_': 1081, 'employee_': 1082, 'stylish_': 1083, 'real_': 1084, 'eat_': 1085, 'emperor_': 1086, 'garlic_': 1087, 'cream_': 1088, 'yellow_': 1089, 'outcome_': 1090, 'lots_': 1091, 'unevenly_': 1092, 'pretty_': 1093, 'violence_': 1094, 'alone_': 1095, 'cloverleaf_': 1096, 'high_': 1097, 'fires_': 1098, 'at_': 1099, 'long_': 1100, 'massage_': 1101, 'suitable_': 1102, 'loved_': 1103, 'mandates_': 1104, 'al_': 1105, 'give_': 1106, 'handed_': 1107, 'clamshell_': 1108, 'go-cart_': 1109, 'parenthood_': 1110, 'drop_': 1111, 'foam_': 1112, 'if_': 1113, 'stung_': 1114, 'buy_': 1115, 'algebraic_': 1116, 'dispensing_': 1117, 'review_': 1118, 'idiotic_': 1119, 'cooking_': 1120, 'bob_': 1121, 'aggressive_': 1122, 'yogurt_': 1123, 'illegal_': 1124, 'obtaining_': 1125, 'dishes_': 1126, 'gold_': 1127, 'bedroom_': 1128, 'move_': 1129, 'coach_': 1130, 'adult_': 1131, 'rise_': 1132, 'prestige_': 1133, 'acclaim_': 1134, 'outage_': 1135, 'monday_': 1136, \"i'll_\": 1137, 'broke_': 1138, 'mammals_': 1139, 'going_': 1140, 'chain_': 1141, 'today_': 1142, 'item_': 1143, 'bleachers_': 1144, 'hauling_': 1145, 'required_': 1146, 'puree_': 1147, 'doctor_': 1148, 'grown_': 1149, 'humid_': 1150, 'overflowed_': 1151, 'gave_': 1152, 'you_': 1153, 'misquote_': 1154, 'our_': 1155, 'whoever_': 1156, 'gives_': 1157, 'outcast_': 1158, 'laugh_': 1159, 'steph_': 1160, 'results_': 1161, 'pleasantly_': 1162, 'wild_': 1163, 'scalp_': 1164, \"didn't_\": 1165, 'another_': 1166, 'misprint_': 1167, 'coyote_': 1168, 'norwegian_': 1169, 'shredded_': 1170, 'top_': 1171, 'regulations_': 1172, 'isotopes_': 1173, 'for_': 1174, 'kindergarten_': 1175, 'bibliographies_': 1176, 'dark_': 1177, 'priorities_': 1178, 'success_': 1179, 'nightly_': 1180, 'accomplished_': 1181, 'ruins_': 1182, 'challenge_': 1183, 'dessert_': 1184, 'determination_': 1185, 'healthy_': 1186, 'day_': 1187, 'worked_': 1188, 'hispanic_': 1189, 'withdraw_': 1190, 'desires_': 1191, 'question_': 1192, 'mediocrity_': 1193, 'vodka_': 1194, 'not_': 1195, 'zig-zagged_': 1196, 'oozed_': 1197, 'begins_': 1198, 'speaker_': 1199, 'co-authors_': 1200, 'particularly_': 1201, 'task_': 1202, 'shore_': 1203, 'rich_': 1204, 'layoffs_': 1205, 'farmland_': 1206, 'oak_': 1207, 'exclusive_': 1208, 'learned_': 1209, 'study_': 1210, 'lunch_': 1211, 'may_': 1212, 'bongos_': 1213, 'oyster_': 1214, 'petrol_': 1215, 'attach_': 1216, 'patient_': 1217, 'tests_': 1218, 'flimsy_': 1219, 'interview_': 1220, 'rob_': 1221, 'about_': 1222, 'needle_': 1223, 'preschooler_': 1224, 'grievances_': 1225, 'special_': 1226, 'cows_': 1227, 'with_': 1228, 'frustration_': 1229, 'abolish_': 1230, 'mine_': 1231, 'holiday_': 1232, 'cottage_': 1233, 'baboon_': 1234, 'earn_': 1235, 'scholars_': 1236, 'months_': 1237, 'voyage_': 1238, 'good_': 1239, 'team_': 1240, 'fresh_': 1241, 'elegant_': 1242, 'future_': 1243, 'screen_': 1244, 'overalls_': 1245, 'goose_': 1246, 'else_': 1247, 'coat_': 1248, 'haunted_': 1249, 'carol_': 1250, 'lives_': 1251, 'experience_': 1252, 'price_': 1253, 'scholastic_': 1254, 'funny_': 1255, 'noise_': 1256, 'big_': 1257, 'harmonize_': 1258, 'harms_': 1259, 'straw_': 1260, 'chew_': 1261, 'steps_': 1262, 'diploma_': 1263, 'companions_': 1264, 'scholar_': 1265, 'vagrants_': 1266, 'twelfth_': 1267, 'them_': 1268, 'word_': 1269, 'sauce_': 1270, 'nearer_': 1271, 'enjoy_': 1272, 'clumsy_': 1273, 'distance_': 1274, 'tom_': 1275, 'celebrate_': 1276, 'tapestry_': 1277, 'national_': 1278, 'pizzerias_': 1279, 'only_': 1280, 'hand_': 1281, \"you'll_\": 1282, 'variety_': 1283, 'an_': 1284, 'cleaners_': 1285, 'gwen_': 1286, 'ices_': 1287, 'tea_': 1288, 'heat_': 1289, 'policeman_': 1290, 'shorten_': 1291, 'innocence_': 1292, 'diseases_': 1293, 'thoroughly_': 1294, 'quality_': 1295, 'snapper_': 1296, 'love_': 1297, 'beach_': 1298, 'tiny_': 1299, 'quick_': 1300, 'book_': 1301, 'lie_': 1302, 'joint_': 1303, 'distributed_': 1304, 'oats_': 1305, 'verse_': 1306, 'her_': 1307, 'remained_': 1308, 'seamstresses_': 1309, 'overly_': 1310, 'let_': 1311, 'create_': 1312, 'beverage_': 1313, 'fifth_': 1314, 'reflects_': 1315, 'parental_': 1316, 'forest_': 1317, 'useful_': 1318, 'ended_': 1319, 'what_': 1320, 'immediate_': 1321, 'cutlery_': 1322, 'housewives_': 1323, 'any_': 1324, 'wounds_': 1325, 'victor_': 1326, 'charge_': 1327, 'cleans_': 1328, 'shrapnel_': 1329, 'every_': 1330, 'roll-ups_': 1331, 'operation_': 1332, 'employment_': 1333, 'vanquished_': 1334, 'squeezed_': 1335, 'apples_': 1336, 'woolen_': 1337, 'although_': 1338, 'slip_': 1339, 'votes_': 1340, \"i'd_\": 1341, 'january_': 1342, 'ahead_': 1343, 'tadpole_': 1344, 'icy_': 1345, 'mercilessly_': 1346, 'departments_': 1347, 'horse_': 1348, 'bagpipes_': 1349, 'tomboy_': 1350, 'sequoia_': 1351, 'tenant_': 1352, 'worship_': 1353, 'could_': 1354, 'old_': 1355, 'feathers_': 1356, 'describe_': 1357, 'pleasure_': 1358, 'pathological_': 1359, 'gunpowder_': 1360, 'butterfly_': 1361, 'engineering_': 1362, 'yearly_': 1363, \"they're_\": 1364, 'shipbuilding_': 1365, 'board_': 1366, 'clamp_': 1367, 'healthier_': 1368, 'guard_': 1369, 'priority_': 1370, 'rationalize_': 1371, 'advertising_': 1372, 'expected_': 1373, 'vanilla_': 1374, 'star_': 1375, 'leaves_': 1376, 'than_': 1377, 'iris_': 1378, 'walking_': 1379, 'sculpture_': 1380, 'challenged_': 1381, 'charged_': 1382, 'greasing_': 1383, 'farmyard_': 1384, 'encyclopedias_': 1385, 'identical_': 1386, 'books_': 1387, 'room_': 1388, 'wire_': 1389, 'thinker_': 1390, 'in_': 1391, 'scared_': 1392, 'male_': 1393, 'fashion_': 1394, 'roy_': 1395, 'morning_': 1396, 'meow_': 1397, 'sherbet_': 1398, 'into_': 1399, 'splurge_': 1400, 'thinks_': 1401, 'audition_': 1402, 'willowy_': 1403, 'brochure_': 1404, 'process_': 1405, 'thought_': 1406, 'sermon_': 1407, 'splurged_': 1408, 'remote_': 1409, 'pewter_': 1410, 'superb_': 1411, 'youngsters_': 1412, 'desperately_': 1413, 'maids_': 1414, 'larger_': 1415, 'crisscrossed_': 1416, 'tradition_': 1417, 'overthrow_': 1418, 'mergers_': 1419, 'come_': 1420, 'bayou_': 1421, 'accusations_': 1422, 'recognition_': 1423, 'computer_': 1424, 'perfume_': 1425, 'courier_': 1426, 'journalist_': 1427, 'third_': 1428, 'without_': 1429, 'lily_': 1430, 'grows_': 1431, 'critical_': 1432, 'gallon_': 1433, 'irate_': 1434, 'problems_': 1435, 'errors_': 1436, 'look_': 1437, 'pill_': 1438, 'getting_': 1439, 'shellfish_': 1440, 'porcupines_': 1441, 'carefully_': 1442, 'oil_': 1443, 'shock_': 1444, 'warm_': 1445, 'autographs_': 1446, 'catch_': 1447, 'reflect_': 1448, 'box_': 1449, 'date_': 1450, 'gloves_': 1451, 'but_': 1452, 'ship_': 1453, 'prepared_': 1454, 'wasp_': 1455, 'competition_': 1456, 'kept_': 1457, 'difficult_': 1458, 'part_': 1459, 'four_': 1460, 'have_': 1461, 'recall_': 1462, 'curry_': 1463, 'however_': 1464, 'forbidden_': 1465, 'thieves_': 1466, 'dig_': 1467, 'cash_': 1468, 'intelligence_': 1469, 'socks_': 1470, 'plant_': 1471, 'peaches_': 1472, 'bowl_': 1473, 'value_': 1474, 'sound_': 1475, 'mosquitoes_': 1476, 'theme_': 1477, 'supervision_': 1478, 'millionaires_': 1479, 'coconut_': 1480, 'driving_': 1481, 'increases_': 1482, 'altruistic_': 1483, 'catastrophic_': 1484, 'like_': 1485, 'official_': 1486, 'authorized_': 1487, 'education_': 1488, 'around_': 1489, 'depicts_': 1490, 'played_': 1491, 'haughty_': 1492, 'upon_': 1493, 'a_': 1494, 'do_': 1495, 'hair_': 1496, 'salad_': 1497, 'pam_': 1498, 'live_': 1499, 'lithographs_': 1500, 'miraculously_': 1501, 'encouraged_': 1502, 'fails_': 1503, 'think_': 1504, 'jeff_': 1505, 'aprons_': 1506, 'away_': 1507, 'tina_': 1508, 'stayed_': 1509, 'tips_': 1510, 'deadline_': 1511, 'dinner_': 1512, 'jar_': 1513, 'tropical_': 1514, 'co-educational_': 1515, 'iguanas_': 1516, 'longer_': 1517, 'me_': 1518, 'line_': 1519, 'dry_': 1520, 'original_': 1521, 'argue_': 1522, 'sold_': 1523, 'goes_': 1524, 'novel_': 1525, 'young_': 1526, 'symposium_': 1527, 'should_': 1528, 'athletic_': 1529, 'go_': 1530, 'parfait_': 1531, 'overcame_': 1532, 'lower_': 1533, \"haven't_\": 1534, 'conditions_': 1535, 'spinach_': 1536, 'bought_': 1537, 'pick_': 1538, 'too_': 1539, 'alleviate_': 1540, 'diminish_': 1541, 'shocked_': 1542, 'talk_': 1543, 'poisonous_': 1544, 'cloth_': 1545, 'charlie_': 1546, 'sing_': 1547, 'shimmers_': 1548, 'wool_': 1549, 'points_': 1550, 'meet_': 1551, 'papered_': 1552, 'got_': 1553, 'chosen_': 1554, 'gets_': 1555, 'entertaining_': 1556, 'sometimes_': 1557, 'beggar_': 1558, 'traffic_': 1559, 'colorful_': 1560, 'cement_': 1561, 'repainted_': 1562, 'journal_': 1563, 'daphne_': 1564, 'raccoons_': 1565, 'situated_': 1566, 'relish_': 1567, 'worm_': 1568, 'lay_': 1569, 'dew_': 1570, 'positive_': 1571, 'birthday_': 1572, 'met_': 1573, 'gardens_': 1574, 'shell_': 1575, 'pledge_': 1576, 'marriage_': 1577, 'service_': 1578, 'strong_': 1579, 'imagination_': 1580, 'accomplish_': 1581, 'shone_': 1582, 'tag_': 1583, 'break_': 1584, 'warrior_': 1585, 'steven_': 1586, 'saturday_': 1587, 'teeth_': 1588, 'flying_': 1589, 'job_': 1590, 'chamber_': 1591, 'cost_': 1592, 'contributions_': 1593, 'foreign_': 1594, 'rabbits_': 1595, 'murky_': 1596, 'eyes_': 1597, 'taxicab_': 1598, 'intuition_': 1599, 'great_': 1600, 'it_': 1601, 'time_': 1602, 'account_': 1603, 'sense_': 1604, 'juicy_': 1605, 'boring_': 1606, 'used_': 1607, 'agree_': 1608, 'ingredients_': 1609, 'leather_': 1610, 'call_': 1611, 'doctors_': 1612, 'clothing_': 1613, 'sitting_': 1614, 'raisins_': 1615, 'surrounded_': 1616, 'eastern_': 1617, 'hallway_': 1618, 'own_': 1619, 'stockings_': 1620, 'change_': 1621, 'attendance_': 1622, 'maintenance_': 1623, 'looked_': 1624, 'organizations_': 1625, 'dwarf_': 1626, 'halloween_': 1627, 'synagogue_': 1628, 'leeway_': 1629, 'naive_': 1630, 'offensive_': 1631, 'hurts_': 1632, 'slopes_': 1633, 'divorced_': 1634, 'alice_': 1635, 'barely_': 1636, 'shows_': 1637, 'presented_': 1638, 'exhibited_': 1639, 'government_': 1640, 'yell_': 1641, 'hung_': 1642, 'centrifuge_': 1643, 'mean_': 1644, 'steve_': 1645, 'solves_': 1646, 'chocolate_': 1647, 'straightforward_': 1648, 'cow_': 1649, 'dutch_': 1650, 'soybeans_': 1651, 'higher_': 1652, 'nevada_': 1653, 'otto_': 1654, 'door_': 1655, 'victim_': 1656, 'ever_': 1657, 'try_': 1658, 'whenever_': 1659, 'apply_': 1660, 'lawyer_': 1661, 'ralph_': 1662, 'audits_': 1663, 'mouse_': 1664, 'general_': 1665, 'gently_': 1666, 'permanent_': 1667, 'antelope_': 1668, 'history_': 1669, 'records_': 1670, 'took_': 1671, 'slowly_': 1672, 'working_': 1673, 'wash_': 1674, 'musicians_': 1675, 'file_': 1676, 'up_': 1677, 'purists_': 1678, 'sweater_': 1679, 'zoo_': 1680, 'frantically_': 1681, 'ski_': 1682, 'pond_': 1683, 'apology_': 1684, 'players_': 1685, 'stomped_': 1686, 'daytime_': 1687, 'while_': 1688, 'enough_': 1689, 'hierarchies_': 1690, 'approval_': 1691, 'children_': 1692, 'freeway_': 1693, 'understanding_': 1694, 'gorgeous_': 1695, 'thirty_': 1696, 'actor_': 1697, 'drink_': 1698, 'eggs_': 1699, 'reorganization_': 1700, 'after_': 1701, 'possible_': 1702, 'to_': 1703, 'backed_': 1704, 'scientific_': 1705, 'music_': 1706, 'did_': 1707, 'canned_': 1708, 'relaxed_': 1709, 'twilight_': 1710, 'surgeon_': 1711, 'tube_': 1712, 'tastes_': 1713, 'personnel_': 1714, 'add_': 1715, 'until_': 1716, 'needlepoint_': 1717, 'thimble_': 1718, 'compounded_': 1719, 'skills_': 1720, 'twins_': 1721, 'proceeding_': 1722, 'chip_': 1723, 'delete_': 1724, 'gigantic_': 1725, 'writers_': 1726, 'purchase_': 1727, 'building_': 1728, 'cannot_': 1729, 'onto_': 1730, 'archeological_': 1731, 'arrange_': 1732, 'gunpoint_': 1733, 'equal_': 1734, 'forces_': 1735, 'web_': 1736, 'upbeat_': 1737, 'pair_': 1738, 'full_': 1739, 'local_': 1740, 'last_': 1741, 'webbed_': 1742, 'techniques_': 1743, 'compile_': 1744, 'dog_': 1745, 'splinter_': 1746, 'ancient_': 1747, 'common_': 1748, 'suburban_': 1749, 'sleigh_': 1750, 'resemble_': 1751, 'examples_': 1752, 'climates_': 1753, 'news_': 1754, 'fudge_': 1755, 'tornados_': 1756, 'under-age_': 1757, 'down_': 1758, 'she_': 1759, 'hostages_': 1760, 'coins_': 1761, 'acropolis_': 1762, 'chives_': 1763, 'clasp_': 1764, 'continental_': 1765, 'spider_': 1766, 'stag_': 1767, 'theory_': 1768, 'roses_': 1769, 'valley_': 1770, 'attention_': 1771, 'coffee_': 1772, 'outlaws_': 1773, 'tooth_': 1774, 'glucose_': 1775, 'sought_': 1776, 'kippers_': 1777, 'biology_': 1778, 'standardized_': 1779, 'football_': 1780, 'ably_': 1781, 'fairy_': 1782, 'count_': 1783, 'ambulance_': 1784, 'rain_': 1785, 'popularity_': 1786, 'very_': 1787, 'almost_': 1788, 'delicious_': 1789, 'proper_': 1790, 'screwdriver_': 1791, 'anecdotal_': 1792, 'notices_': 1793, 'subject_': 1794, 'auditory_': 1795, 'thoughtless_': 1796, 'system_': 1797, 'breakdown_': 1798, 'loads_': 1799, 'pine_': 1800, 'found_': 1801, 'illuminating_': 1802, 'trouble_': 1803, 'subtraction_': 1804, 'allow_': 1805}\n"
     ]
    }
   ],
   "source": [
    "from ecog2txt_pytorch.vocabulary import Vocabulary\n",
    "\n",
    "word_seq_vocabulary = Vocabulary(\"/Users/akshita/Documents/Research/Makin/ecog2txt-pytorch/conf/vocab.mocha-timit.1806\")\n",
    "SRC_VOCAB_SIZE = num_channels\n",
    "TGT_VOCAB_SIZE = len(word_seq_vocabulary.words_ind_map)\n",
    "EMB_SIZE = num_channels\n",
    "PAD_IDX = word_seq_vocabulary.words_ind_map['<pad>']\n",
    "EOS_IDX = word_seq_vocabulary.words_ind_map['<EOS>']\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "WIN_SIZE = 1\n",
    "\n",
    "block_config_all = None\n",
    "with open(block_config_path) as bf:\n",
    "    block_config_all = json.load(bf)\n",
    "\n",
    "description = {\"audio_sequence\": \"float\", \"ecog_sequence\": \"float\",\n",
    "               \"text_sequence\": \"byte\", \"phoneme_sequence\": \"byte\"}\n",
    "ecog = EcogDataLoader(tfrecord_path, block_config_all[subject_id],\n",
    "                      subject_id, num_ECoG_channels=num_channels, description=description)\n",
    "#print(ecog.get_data_loader_for_blocks())\n",
    "\n",
    "\n",
    "train_iter = iter(ecog.get_data_loader_for_blocks(batch_size=BATCH_SIZE, partition_type='training'))\n",
    "test_iter = iter(ecog.get_data_loader_for_blocks(batch_size=BATCH_SIZE, partition_type='extra'))\n",
    "valid_iter = iter(ecog.get_data_loader_for_blocks(batch_size=BATCH_SIZE, partition_type='validation'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from ecog2txt_pytorch.models.single_subject_transformer import *\n",
    "from longformer.longformer import LongformerSelfAttention, LongformerConfig\n",
    "\n",
    "\n",
    "longformer_config = LongformerConfig(attention_window=[WIN_SIZE] * NUM_ENCODER_LAYERS,\n",
    " attention_dilation=[1] * NUM_ENCODER_LAYERS,\n",
    " hidden_size=EMB_SIZE,\n",
    " num_attention_heads=NHEAD)\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
    " EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
    " FFN_HID_DIM)\n",
    "\n",
    "for i, layer in enumerate(transformer.transformer_encoder.layers):\n",
    " layer.self_attn.self = LongformerSelfAttention(config=longformer_config, layer_id=i)\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "  src_seq_len = src.shape[0]\n",
    "  tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "  tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "  src_mask = torch.zeros((src_seq_len, src_seq_len), device=device)\n",
    "  #making a mask with sliding window centred around i\n",
    "  # ind_src = torch.arange(src_seq_len+WIN_SIZE-1, dtype=torch.int64).unfold(0,WIN_SIZE,1) - WIN_SIZE/2\n",
    "  # ind_src[ind_src<0] = 0\n",
    "  # ind_src[ind_src>=src_seq_len] = src_seq_len - 1\n",
    "  # ind_src = ind_src.type(torch.int64)\n",
    "  # src_mask.scatter_(1,ind_src,1)\n",
    "  # print('SRC_MASK', src_mask, 'SRC_mask_shape', src_mask.shape)\n",
    "  # src_mask = src_mask.float().masked_fill(src_mask == 0, float('-inf')).masked_fill(src_mask == 1, float(0.0))\n",
    "\n",
    "  src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "  tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "  return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "def train_epoch(model, train_iter, optimizer):\n",
    "  model.train()\n",
    "  losses = 0\n",
    "  for idx, (src, tgt) in enumerate(train_iter):\n",
    "      print('src_shape', src.shape, 'tgt_shape', tgt.shape)\n",
    "      src = src.to(device)\n",
    "      tgt = tgt.to(device)\n",
    "\n",
    "      tgt_input = tgt[:-1, :]\n",
    "\n",
    "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "      logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    "                                src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "      print(\"af model\")\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      tgt_out = tgt[1:,:]\n",
    "\n",
    "      loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "      print(\"after loss\", loss)\n",
    "      loss.backward()\n",
    "      print(\"after loss back\")\n",
    "      optimizer.step()\n",
    "      losses += loss.item()\n",
    "      print(\"losses\", losses)\n",
    "  return losses / len(train_iter)\n",
    "\n",
    "\n",
    "def evaluate(model, val_iter):\n",
    "  model.eval()\n",
    "  losses = 0\n",
    "  for idx, (src, tgt) in (enumerate(valid_iter)):\n",
    "    src = src.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "\n",
    "    tgt_input = tgt[:-1, :]\n",
    "\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "    logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    "                              src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "    tgt_out = tgt[1:,:]\n",
    "    loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "    losses += loss.item()\n",
    "  return losses / len(val_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_shape torch.Size([1250, 128, 448]) tgt_shape torch.Size([13, 128])\n",
      "tokens shape:  torch.Size([12, 128])\n",
      "src_emb_shape:  torch.Size([1250, 128, 448]) tgt_emb_shape:  torch.Size([12, 128, 448]) src_mask_shape:  torch.Size([1250, 1250]) src_pad_shape:  torch.Size([128, 1250, 448])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-a0e3c73f8ba3>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mNUM_EPOCHS\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m   \u001B[0mstart_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m   \u001B[0mtrain_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_epoch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtransformer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_iter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m   \u001B[0mend_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m   \u001B[0mval_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mevaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtransformer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid_iter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-6-42898baaf3b2>\u001B[0m in \u001B[0;36mtrain_epoch\u001B[0;34m(model, train_iter, optimizer)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m       logits = model(src, tgt_input, src_mask, tgt_mask,\n\u001B[0;32m---> 38\u001B[0;31m                                 src_padding_mask, tgt_padding_mask, src_padding_mask)\n\u001B[0m\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m       \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"af model\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Research/research_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Research/Makin/ecog2txt-pytorch/ecog2txt_pytorch/models/single_subject_transformer.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\u001B[0m\n\u001B[1;32m     41\u001B[0m         print('src_emb_shape: ', src_emb.shape,'tgt_emb_shape: ',tgt_emb.shape ,'src_mask_shape: ',\n\u001B[1;32m     42\u001B[0m               src_mask.shape, 'src_pad_shape: ',src_padding_mask.shape)\n\u001B[0;32m---> 43\u001B[0;31m         \u001B[0mmemory\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransformer_encoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc_emb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"here af tr enc\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
      "\u001B[0;32m~/Documents/Research/research_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Research/research_env/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, src, mask, src_key_padding_mask)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    180\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmod\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 181\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_key_padding_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msrc_key_padding_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    182\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    183\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Research/research_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Research/research_env/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, src, src_mask, src_key_padding_mask)\u001B[0m\n\u001B[1;32m    292\u001B[0m         \"\"\"\n\u001B[1;32m    293\u001B[0m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001B[0;32m--> 294\u001B[0;31m                               key_padding_mask=src_key_padding_mask)[0]\n\u001B[0m\u001B[1;32m    295\u001B[0m         \u001B[0msrc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msrc\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    296\u001B[0m         \u001B[0msrc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Research/research_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Research/research_env/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001B[0m\n\u001B[1;32m    985\u001B[0m                 \u001B[0mtraining\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    986\u001B[0m                 \u001B[0mkey_padding_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkey_padding_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mneed_weights\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mneed_weights\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 987\u001B[0;31m                 attn_mask=attn_mask)\n\u001B[0m\u001B[1;32m    988\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    989\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Research/research_env/lib/python3.7/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mmulti_head_attention_forward\u001B[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001B[0m\n\u001B[1;32m   4805\u001B[0m         \u001B[0mattn_output_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mattn_output_weights\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbsz\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mnum_heads\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtgt_len\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_len\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4806\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4807\u001B[0;31m     \u001B[0mattn_output_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msoftmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattn_output_weights\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4808\u001B[0m     \u001B[0mattn_output_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattn_output_weights\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdropout_p\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtraining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4809\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Research/research_env/lib/python3.7/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36msoftmax\u001B[0;34m(input, dim, _stacklevel, dtype)\u001B[0m\n\u001B[1;32m   1581\u001B[0m         \u001B[0mdim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_softmax_dim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"softmax\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_stacklevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1582\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mdtype\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1583\u001B[0;31m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msoftmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1584\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1585\u001B[0m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msoftmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "  start_time = time.time()\n",
    "  train_loss = train_epoch(transformer, train_iter, optimizer)\n",
    "  end_time = time.time()\n",
    "  val_loss = evaluate(transformer, valid_iter)\n",
    "  print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "          f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
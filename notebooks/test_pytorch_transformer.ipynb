{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc11cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"403\"\n",
    "\n",
    "manifest_path=\"/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/conf/mocha-1_word_sequence.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d02d1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package conflict (probably because you are using TF2.x)...not loading tfmpl...\n",
      "Warning: package 'samplerate' not found; skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/ecog2txt_pytorch/trainers/single_subject_trainer.py:35: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  manifest_file = yaml.load(f)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makamsali\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">vital-silence-46</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/akamsali/ecog2txt-pytorch\" target=\"_blank\">https://wandb.ai/akamsali/ecog2txt-pytorch</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/akamsali/ecog2txt-pytorch/runs/3ghogxtt\" target=\"_blank\">https://wandb.ai/akamsali/ecog2txt-pytorch/runs/3ghogxtt</a><br/>\n",
       "                Run data is saved locally in <code>/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/notebooks/wandb/run-20210713_021203-3ghogxtt</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 6.152, Val loss: 5.140, Val WER: 0.727 Epoch time = 10.629s\n",
      "Epoch: 2, Train loss: 5.538, Val loss: 4.706, Val WER: 0.727 Epoch time = 10.061s\n",
      "Epoch: 3, Train loss: 5.251, Val loss: 4.305, Val WER: 0.719 Epoch time = 10.149s\n",
      "Epoch: 4, Train loss: 4.929, Val loss: 3.847, Val WER: 0.660 Epoch time = 9.969s\n",
      "Epoch: 5, Train loss: 4.630, Val loss: 3.473, Val WER: 0.632 Epoch time = 10.099s\n",
      "Epoch: 6, Train loss: 4.355, Val loss: 3.113, Val WER: 0.554 Epoch time = 9.955s\n",
      "Epoch: 7, Train loss: 4.102, Val loss: 2.841, Val WER: 0.484 Epoch time = 9.981s\n",
      "Epoch: 8, Train loss: 3.865, Val loss: 2.545, Val WER: 0.449 Epoch time = 10.050s\n",
      "Epoch: 9, Train loss: 3.646, Val loss: 2.350, Val WER: 0.429 Epoch time = 9.915s\n",
      "Epoch: 10, Train loss: 3.439, Val loss: 2.148, Val WER: 0.403 Epoch time = 9.931s\n",
      "Epoch: 11, Train loss: 3.245, Val loss: 1.981, Val WER: 0.368 Epoch time = 9.939s\n",
      "Epoch: 12, Train loss: 3.076, Val loss: 1.846, Val WER: 0.347 Epoch time = 10.107s\n",
      "Epoch: 13, Train loss: 2.910, Val loss: 1.734, Val WER: 0.329 Epoch time = 9.948s\n",
      "Epoch: 14, Train loss: 2.768, Val loss: 1.633, Val WER: 0.310 Epoch time = 9.945s\n",
      "Epoch: 15, Train loss: 2.631, Val loss: 1.524, Val WER: 0.302 Epoch time = 9.929s\n",
      "Epoch: 16, Train loss: 2.523, Val loss: 1.479, Val WER: 0.293 Epoch time = 9.962s\n",
      "Epoch: 17, Train loss: 2.458, Val loss: 1.491, Val WER: 0.309 Epoch time = 9.933s\n",
      "Epoch: 18, Train loss: 2.339, Val loss: 1.336, Val WER: 0.248 Epoch time = 9.964s\n",
      "Epoch: 19, Train loss: 2.198, Val loss: 1.308, Val WER: 0.274 Epoch time = 10.058s\n",
      "Epoch: 20, Train loss: 2.092, Val loss: 1.241, Val WER: 0.238 Epoch time = 10.088s\n",
      "Epoch: 21, Train loss: 1.995, Val loss: 1.209, Val WER: 0.244 Epoch time = 9.898s\n",
      "Epoch: 22, Train loss: 1.910, Val loss: 1.175, Val WER: 0.226 Epoch time = 9.898s\n",
      "Epoch: 23, Train loss: 1.822, Val loss: 1.089, Val WER: 0.215 Epoch time = 9.981s\n",
      "Epoch: 24, Train loss: 1.740, Val loss: 1.057, Val WER: 0.209 Epoch time = 9.967s\n",
      "Epoch: 25, Train loss: 1.648, Val loss: 1.030, Val WER: 0.206 Epoch time = 9.890s\n",
      "Epoch: 26, Train loss: 1.567, Val loss: 1.010, Val WER: 0.200 Epoch time = 9.869s\n",
      "Epoch: 27, Train loss: 1.489, Val loss: 0.989, Val WER: 0.192 Epoch time = 9.973s\n",
      "Epoch: 28, Train loss: 1.445, Val loss: 0.951, Val WER: 0.167 Epoch time = 10.004s\n",
      "Epoch: 29, Train loss: 1.353, Val loss: 0.844, Val WER: 0.131 Epoch time = 9.995s\n",
      "Epoch: 30, Train loss: 1.278, Val loss: 0.840, Val WER: 0.145 Epoch time = 9.955s\n",
      "Epoch: 31, Train loss: 1.203, Val loss: 0.795, Val WER: 0.128 Epoch time = 9.985s\n",
      "Epoch: 32, Train loss: 1.131, Val loss: 0.768, Val WER: 0.128 Epoch time = 10.005s\n",
      "Epoch: 33, Train loss: 1.065, Val loss: 0.753, Val WER: 0.134 Epoch time = 9.881s\n",
      "Epoch: 34, Train loss: 1.000, Val loss: 0.743, Val WER: 0.134 Epoch time = 9.934s\n",
      "Epoch: 35, Train loss: 0.941, Val loss: 0.717, Val WER: 0.134 Epoch time = 9.967s\n",
      "Epoch: 36, Train loss: 0.880, Val loss: 0.723, Val WER: 0.130 Epoch time = 9.976s\n",
      "Epoch: 37, Train loss: 0.828, Val loss: 0.700, Val WER: 0.131 Epoch time = 9.878s\n",
      "Epoch: 38, Train loss: 0.768, Val loss: 0.693, Val WER: 0.138 Epoch time = 9.937s\n",
      "Epoch: 39, Train loss: 0.726, Val loss: 0.697, Val WER: 0.133 Epoch time = 10.060s\n",
      "Epoch: 40, Train loss: 0.665, Val loss: 0.676, Val WER: 0.134 Epoch time = 10.011s\n",
      "Epoch: 41, Train loss: 0.631, Val loss: 0.703, Val WER: 0.135 Epoch time = 9.917s\n",
      "Epoch: 42, Train loss: 0.585, Val loss: 0.698, Val WER: 0.140 Epoch time = 9.928s\n",
      "Epoch: 43, Train loss: 0.534, Val loss: 0.657, Val WER: 0.126 Epoch time = 9.975s\n",
      "Epoch: 44, Train loss: 0.501, Val loss: 0.596, Val WER: 0.116 Epoch time = 10.001s\n",
      "Epoch: 45, Train loss: 0.471, Val loss: 0.630, Val WER: 0.126 Epoch time = 9.833s\n",
      "Epoch: 46, Train loss: 0.436, Val loss: 0.624, Val WER: 0.124 Epoch time = 9.917s\n",
      "Epoch: 47, Train loss: 0.399, Val loss: 0.604, Val WER: 0.123 Epoch time = 9.939s\n",
      "Epoch: 48, Train loss: 0.369, Val loss: 0.625, Val WER: 0.124 Epoch time = 9.985s\n",
      "Epoch: 49, Train loss: 0.336, Val loss: 0.558, Val WER: 0.100 Epoch time = 9.937s\n",
      "Epoch: 50, Train loss: 0.322, Val loss: 0.638, Val WER: 0.123 Epoch time = 10.000s\n",
      "Epoch: 51, Train loss: 0.297, Val loss: 0.684, Val WER: 0.135 Epoch time = 10.052s\n",
      "Epoch: 52, Train loss: 0.272, Val loss: 0.576, Val WER: 0.117 Epoch time = 9.918s\n",
      "Epoch: 53, Train loss: 0.248, Val loss: 0.580, Val WER: 0.109 Epoch time = 9.884s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d453b1811189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleSubjectTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanifest_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanifest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/ecog2txt_pytorch/trainers/single_subject_trainer.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_wer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/ecog2txt_pytorch/trainers/single_subject_trainer.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mtgt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ecog2txt_pytorch.trainers.single_subject_trainer import SingleSubjectTrainer\n",
    "\n",
    "trainer = SingleSubjectTrainer(subject_id=subject_id, manifest_path=manifest_path)\n",
    "\n",
    "trainer.train_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

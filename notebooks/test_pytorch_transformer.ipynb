{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc11cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"400\"\n",
    "\n",
    "manifest_path=\"/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/conf/mocha-1_word_sequence.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d02d1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package conflict (probably because you are using TF2.x)...not loading tfmpl...\n",
      "Warning: package 'samplerate' not found; skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/ecog2txt_pytorch/trainers/single_subject_trainer.py:35: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  manifest_file = yaml.load(f)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makamsali\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">honest-forest-42</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/akamsali/ecog2txt-pytorch\" target=\"_blank\">https://wandb.ai/akamsali/ecog2txt-pytorch</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/akamsali/ecog2txt-pytorch/runs/3ba7012l\" target=\"_blank\">https://wandb.ai/akamsali/ecog2txt-pytorch/runs/3ba7012l</a><br/>\n",
       "                Run data is saved locally in <code>/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/notebooks/wandb/run-20210713_004910-3ba7012l</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 7.259, Val loss: 7.027, Val WER: 0.980 Epoch time = 8.042s\n",
      "Epoch: 2, Train loss: 6.955, Val loss: 6.876, Val WER: 0.980 Epoch time = 7.403s\n",
      "Epoch: 3, Train loss: 6.807, Val loss: 6.723, Val WER: 0.980 Epoch time = 7.378s\n",
      "Epoch: 4, Train loss: 6.649, Val loss: 6.553, Val WER: 0.968 Epoch time = 7.349s\n",
      "Epoch: 5, Train loss: 6.479, Val loss: 6.331, Val WER: 0.950 Epoch time = 7.437s\n",
      "Epoch: 6, Train loss: 6.299, Val loss: 6.141, Val WER: 0.943 Epoch time = 7.318s\n",
      "Epoch: 7, Train loss: 6.107, Val loss: 5.941, Val WER: 0.928 Epoch time = 7.368s\n",
      "Epoch: 8, Train loss: 5.919, Val loss: 5.763, Val WER: 0.930 Epoch time = 7.373s\n",
      "Epoch: 9, Train loss: 5.734, Val loss: 5.545, Val WER: 0.894 Epoch time = 7.277s\n",
      "Epoch: 10, Train loss: 5.567, Val loss: 5.419, Val WER: 0.889 Epoch time = 7.371s\n",
      "Epoch: 11, Train loss: 5.389, Val loss: 5.205, Val WER: 0.844 Epoch time = 7.435s\n",
      "Epoch: 12, Train loss: 5.218, Val loss: 5.020, Val WER: 0.867 Epoch time = 7.306s\n",
      "Epoch: 13, Train loss: 5.040, Val loss: 4.856, Val WER: 0.828 Epoch time = 7.283s\n",
      "Epoch: 14, Train loss: 4.885, Val loss: 4.677, Val WER: 0.842 Epoch time = 7.343s\n",
      "Epoch: 15, Train loss: 4.720, Val loss: 4.494, Val WER: 0.805 Epoch time = 7.379s\n",
      "Epoch: 16, Train loss: 4.557, Val loss: 4.384, Val WER: 0.782 Epoch time = 7.324s\n",
      "Epoch: 17, Train loss: 4.400, Val loss: 4.243, Val WER: 0.795 Epoch time = 7.287s\n",
      "Epoch: 18, Train loss: 4.252, Val loss: 4.057, Val WER: 0.736 Epoch time = 7.347s\n",
      "Epoch: 19, Train loss: 4.105, Val loss: 3.945, Val WER: 0.712 Epoch time = 7.292s\n",
      "Epoch: 20, Train loss: 3.971, Val loss: 3.783, Val WER: 0.659 Epoch time = 7.289s\n",
      "Epoch: 21, Train loss: 3.827, Val loss: 3.648, Val WER: 0.634 Epoch time = 7.548s\n",
      "Epoch: 22, Train loss: 3.674, Val loss: 3.489, Val WER: 0.521 Epoch time = 7.358s\n",
      "Epoch: 23, Train loss: 3.544, Val loss: 3.364, Val WER: 0.473 Epoch time = 7.255s\n",
      "Epoch: 24, Train loss: 3.411, Val loss: 3.207, Val WER: 0.418 Epoch time = 7.285s\n",
      "Epoch: 25, Train loss: 3.270, Val loss: 3.113, Val WER: 0.448 Epoch time = 7.268s\n",
      "Epoch: 26, Train loss: 3.143, Val loss: 2.971, Val WER: 0.433 Epoch time = 7.394s\n",
      "Epoch: 27, Train loss: 3.025, Val loss: 2.899, Val WER: 0.424 Epoch time = 7.320s\n",
      "Epoch: 28, Train loss: 2.894, Val loss: 2.752, Val WER: 0.381 Epoch time = 7.315s\n",
      "Epoch: 29, Train loss: 2.756, Val loss: 2.643, Val WER: 0.367 Epoch time = 7.308s\n",
      "Epoch: 30, Train loss: 2.644, Val loss: 2.502, Val WER: 0.301 Epoch time = 7.320s\n",
      "Epoch: 31, Train loss: 2.538, Val loss: 2.403, Val WER: 0.283 Epoch time = 7.443s\n",
      "Epoch: 32, Train loss: 2.418, Val loss: 2.345, Val WER: 0.278 Epoch time = 7.279s\n",
      "Epoch: 33, Train loss: 2.314, Val loss: 2.218, Val WER: 0.236 Epoch time = 7.276s\n",
      "Epoch: 34, Train loss: 2.195, Val loss: 2.096, Val WER: 0.204 Epoch time = 7.284s\n",
      "Epoch: 35, Train loss: 2.091, Val loss: 2.029, Val WER: 0.169 Epoch time = 7.265s\n",
      "Epoch: 36, Train loss: 1.988, Val loss: 1.964, Val WER: 0.180 Epoch time = 7.273s\n",
      "Epoch: 37, Train loss: 1.885, Val loss: 1.854, Val WER: 0.170 Epoch time = 7.543s\n",
      "Epoch: 38, Train loss: 1.782, Val loss: 1.767, Val WER: 0.171 Epoch time = 7.305s\n",
      "Epoch: 39, Train loss: 1.685, Val loss: 1.736, Val WER: 0.190 Epoch time = 7.254s\n",
      "Epoch: 40, Train loss: 1.606, Val loss: 1.636, Val WER: 0.149 Epoch time = 7.251s\n",
      "Epoch: 41, Train loss: 1.508, Val loss: 1.570, Val WER: 0.149 Epoch time = 7.274s\n",
      "Epoch: 42, Train loss: 1.424, Val loss: 1.518, Val WER: 0.138 Epoch time = 7.505s\n",
      "Epoch: 43, Train loss: 1.337, Val loss: 1.452, Val WER: 0.133 Epoch time = 7.352s\n",
      "Epoch: 44, Train loss: 1.255, Val loss: 1.343, Val WER: 0.138 Epoch time = 7.334s\n",
      "Epoch: 45, Train loss: 1.182, Val loss: 1.299, Val WER: 0.122 Epoch time = 7.311s\n",
      "Epoch: 46, Train loss: 1.104, Val loss: 1.257, Val WER: 0.127 Epoch time = 7.278s\n",
      "Epoch: 47, Train loss: 1.027, Val loss: 1.154, Val WER: 0.122 Epoch time = 7.535s\n",
      "Epoch: 48, Train loss: 0.960, Val loss: 1.126, Val WER: 0.111 Epoch time = 7.254s\n",
      "Epoch: 49, Train loss: 0.893, Val loss: 1.090, Val WER: 0.106 Epoch time = 7.279s\n",
      "Epoch: 50, Train loss: 0.825, Val loss: 1.075, Val WER: 0.111 Epoch time = 7.259s\n",
      "Epoch: 51, Train loss: 0.772, Val loss: 0.993, Val WER: 0.093 Epoch time = 7.225s\n",
      "Epoch: 52, Train loss: 0.713, Val loss: 0.984, Val WER: 0.108 Epoch time = 7.258s\n",
      "Epoch: 53, Train loss: 0.662, Val loss: 0.919, Val WER: 0.096 Epoch time = 7.363s\n",
      "Epoch: 54, Train loss: 0.616, Val loss: 0.878, Val WER: 0.088 Epoch time = 7.306s\n",
      "Epoch: 55, Train loss: 0.568, Val loss: 0.854, Val WER: 0.085 Epoch time = 7.232s\n",
      "Epoch: 56, Train loss: 0.522, Val loss: 0.808, Val WER: 0.067 Epoch time = 7.261s\n",
      "Epoch: 57, Train loss: 0.478, Val loss: 0.810, Val WER: 0.084 Epoch time = 7.275s\n",
      "Epoch: 58, Train loss: 0.443, Val loss: 0.768, Val WER: 0.104 Epoch time = 7.455s\n",
      "Epoch: 59, Train loss: 0.401, Val loss: 0.746, Val WER: 0.089 Epoch time = 7.384s\n",
      "Epoch: 60, Train loss: 0.370, Val loss: 0.726, Val WER: 0.084 Epoch time = 7.304s\n",
      "Epoch: 61, Train loss: 0.340, Val loss: 0.670, Val WER: 0.067 Epoch time = 7.307s\n",
      "Epoch: 62, Train loss: 0.309, Val loss: 0.668, Val WER: 0.084 Epoch time = 7.288s\n",
      "Epoch: 63, Train loss: 0.285, Val loss: 0.643, Val WER: 0.063 Epoch time = 7.566s\n",
      "Epoch: 64, Train loss: 0.260, Val loss: 0.628, Val WER: 0.063 Epoch time = 7.273s\n",
      "Epoch: 65, Train loss: 0.237, Val loss: 0.612, Val WER: 0.071 Epoch time = 7.316s\n",
      "Epoch: 66, Train loss: 0.216, Val loss: 0.612, Val WER: 0.080 Epoch time = 7.212s\n",
      "Epoch: 67, Train loss: 0.193, Val loss: 0.573, Val WER: 0.072 Epoch time = 7.244s\n",
      "Epoch: 68, Train loss: 0.177, Val loss: 0.548, Val WER: 0.066 Epoch time = 7.252s\n",
      "Epoch: 69, Train loss: 0.163, Val loss: 0.547, Val WER: 0.066 Epoch time = 7.434s\n",
      "Epoch: 70, Train loss: 0.148, Val loss: 0.534, Val WER: 0.071 Epoch time = 7.302s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d453b1811189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleSubjectTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanifest_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanifest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/ecog2txt_pytorch/trainers/single_subject_trainer.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_wer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/Makin/ecog2txt-pytorch/ecog2txt_pytorch/trainers/single_subject_trainer.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/research_env/lib64/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gilbreth/akamsali/Research/research_env/lib64/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ecog2txt_pytorch.trainers.single_subject_trainer import SingleSubjectTrainer\n",
    "\n",
    "trainer = SingleSubjectTrainer(subject_id=subject_id, manifest_path=manifest_path)\n",
    "\n",
    "trainer.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "201840//216"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
